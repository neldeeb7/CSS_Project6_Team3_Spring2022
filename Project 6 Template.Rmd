---
title: 'Project 6: Randomization and Matching'
output: pdf_document
---

# Introduction

In this project, you will explore the question of whether college education causally affects political participation. Specifically, you will use replication data from \href{https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1409483}{Who Matches? Propensity Scores and Bias in the Causal Eï¬€ects of Education on Participation} by former Berkeley PhD students John Henderson and Sara Chatfield. Their paper is itself a replication study of \href{https://www.jstor.org/stable/10.1017/s0022381608080651}{Reconsidering the Effects of Education on Political Participation} by Cindy Kam and Carl Palmer. In their original 2008 study, Kam and Palmer argue that college education has no effect on later political participation, and use the propensity score matching to show that pre-college political activity drives selection into college and later political participation. Henderson and Chatfield in their 2011 paper argue that the use of the propensity score matching in this context is inappropriate because of the bias that arises from small changes in the choice of variables used to model the propensity score. They use \href{http://sekhon.berkeley.edu/papers/GenMatch.pdf}{genetic matching} (at that point a new method), which uses an approach similar to optimal matching to optimize Mahalanobis distance weights. Even with genetic matching, they find that balance remains elusive however, thus leaving open the question of whether education causes political participation.

You will use these data and debates to investigate the benefits and pitfalls associated with matching methods. Replication code for these papers is available online, but as you'll see, a lot has changed in the last decade or so of data science! Throughout the assignment, use tools we introduced in lab from the \href{https://www.tidyverse.org/}{tidyverse} and the \href{https://cran.r-project.org/web/packages/MatchIt/MatchIt.pdf}{MatchIt} packages. Specifically, try to use dplyr, tidyr, purrr, stringr, and ggplot instead of base R functions. While there are other matching software libraries available, MatchIt tends to be the most up to date and allows for consistent syntax.

# Data

The data is drawn from the \href{https://www.icpsr.umich.edu/web/ICPSR/studies/4023/datadocumentation#}{Youth-Parent Socialization Panel Study} which asked students and parents a variety of questions about their political participation. This survey was conducted in several waves. The first wave was in 1965 and established the baseline pre-treatment covariates. The treatment is whether the student attended college between 1965 and 1973 (the time when the next survey wave was administered). The outcome is an index that calculates the number of political activities the student engaged in after 1965. Specifically, the key variables in this study are:

\begin{itemize}
    \item \textbf{college}: Treatment of whether the student attended college or not. 1 if the student attended college between 1965 and 1973, 0 otherwise.
    \item \textbf{ppnscal}: Outcome variable measuring the number of political activities the student participated in. Additive combination of whether the student voted in 1972 or 1980 (student\_vote), attended a campaign rally or meeting (student\_meeting), wore a campaign button (student\_button), donated money to a campaign (student\_money), communicated with an elected official (student\_communicate), attended a demonstration or protest (student\_demonstrate), was involved with a local community event (student\_community), or some other political participation (student\_other)
\end{itemize}

Otherwise, we also have covariates measured for survey responses to various questions about political attitudes. We have covariates measured for the students in the baseline year, covariates for their parents in the baseline year, and covariates from follow-up surveys. \textbf{Be careful here}. In general, post-treatment covariates will be clear from the name (i.e. student\_1973Married indicates whether the student was married in the 1973 survey). Be mindful that the baseline covariates were all measured in 1965, the treatment occurred between 1965 and 1973, and the outcomes are from 1973 and beyond. We will distribute the Appendix from Henderson and Chatfield that describes the covariates they used, but please reach out with any questions if you have questions about what a particular variable means.

```{r}
# Load tidyverse and MatchIt
# Feel free to load other libraries as you wish
library(tidyverse)
library(MatchIt)
library(ggplot2)
library(dplyr)

set.seed(10)

# Load ypsps data
ypsps <- read_csv('data/ypsps.csv')
head(ypsps)
```

# Randomization

Matching is usually used in observational studies to to approximate random assignment to treatment. But could it be useful even in randomized studies? To explore the question do the following:

\begin{enumerate}
    \item Generate a vector that randomly assigns each unit to either treatment or control
    \item Choose a baseline covariate (for either the student or parent). A binary covariate is probably best for this exercise.
    \item Visualize the distribution of the covariate by treatment/control condition. Are treatment and control balanced on this covariate?
    \item Simulate the first 3 steps 10,000 times and visualize the distribution of treatment/control balance across the simulations.
\end{enumerate}

```{r}
# Generate a vector that randomly assigns each unit to treatment/control
rdm_vtr <- sample(0:1, size = nrow(ypsps), replace = TRUE, prob = c(0.5,0.5)) ## 0 = Control; 1 = Treatment

# Choose a baseline covariate (use dplyr for this)
sample_df = ypsps %>% 
  select(student_demonstrate) %>% 
  mutate(rdm_vtr)

# Visualize the distribution by treatment/control (ggplot)
ggplot(sample_df, aes(x = student_demonstrate, fill = factor(rdm_vtr))) + 
  geom_bar() +
  facet_grid(rdm_vtr~.) +
  labs(title = "Distribution of Student Participation in Demonstrations", fill = "Random \nAssignment\n")

# Simulate this 10,000 times
distr_vec <- vector()

for (iter_num in 1:10000) {  
   rdm_vtr <- sample(0:1, size = nrow(ypsps), replace = TRUE, prob = c(0.5,0.5))
   
   sample_df = ypsps %>% 
     select(student_demonstrate) %>% 
     mutate(rdm_vtr)
   
   distr_vec <- c(distr_vec, 
                  (sum(sample_df[which(sample_df$rdm_vtr==1), 1]) / nrow(sample_df))
                  ) 
}
ggplot() + 
  geom_histogram(aes(distr_vec))

```

## Questions
\begin{enumerate}
    \item \textbf{What do you see across your simulations? Why does independence of treatment assignment and baseline covariates not guarantee balance of treatment assignment and baseline covariates?}
\end{enumerate}

\textbf{Your Answer}: From the histogram, it appears that there is a normal distribution of students engaging in political demonstrations across the control/treatment classes. This normal distribution indicates that there is an independence between the selected covariate and the exposure variable. However, we can not assume independence of treatment assignment because this is an observational study, not an experimental study.


# Propensity Score Matching

## One Model
Select covariates that you think best represent the "true" model predicting whether a student chooses to attend college, and estimate a p # generate summary information for the matching model, extract percent improvement, and save in df

```{r}
# Select covariates that represent the "true" model for selection into college
sel_college_vars <- c("student_GPA", "student_SchOfficer", "student_SchClub", "student_SchOfficer", "student_SchPublish", "student_Hobby", "student_SchClub", "student_OccClub", "student_NeighClub", "student_RelClub", "student_YouthOrg", "student_MiscClub", "student_ClubLev", "student_Knowledge", "student_FPlans", "student_Newspaper", "student_Magazine", "student_Gen", "student_Race", "parent_EducHH", "parent_EducW", "parent_HHInc", "parent_Knowledge")

##Logic for choosing the above covariates:
# student's GPA and school involvement are part of admissions
# student knowledge & whether they finish plans
# students' desire to read probably influences future plans for education
# parental educ, knowledge, and income matter
# gender and race of student influences college attendance


# Fit model with matching
glm_formula <- paste("college ~ ",paste(sel_college_vars, collapse=" + "))
matching_ps_att <- matchit(formula = formula(glm_formula),
                            data = ypsps,
                            method = "nearest",
                            distance = "glm",
                            link = "logit",
                            estimand = "ATT",
                            replace = TRUE, 
                            #because there is imbalance in the number of college attendees and non-attendees
                            ratio = 2)


# Save data from model for later
matching_ps_att_data <- match.data(matching_ps_att)
```


```{r}
# Report the overall balance and the proportion of covariates that meet the balance threshold

# Install cobalt
install.packages("cobalt")
library(cobalt)

# Use cobalt to create a balance table
balance_table <- bal.tab(x = matching_ps_att, thresholds = .1)
balance_table
# Only 12 out of 22 covariates are balanced post-matching
# We have 306 controls matched to 803 treated. 145 controls had no match.

summary(matching_ps_att) # this shows percent balance improvement post-matching
# Balance got worse for these covariates after matching:
## student_SchClub                    (-16.6%)
## student_Hobby                      (-588.3%)
## student_Race                       (-8.2%)
# However, student_Race is still balanced in the end, meeting the threshold

# Examine and plot the balance for the top 10 covariates
# First, create a data frame with covariates, their coefficients' absolute values, and keep only the ones with the largest coefs
top_coef_df <- data.frame(abs(matching_ps_att$model$coefficients)) %>%
  slice_max(abs.matching_ps_att.model.coefficients., n = 11) #11, not 10, to include the intercept AND 10 covariates
# Save the names of the top 10 covariates.
top_coefs <- c(rownames(top_coef_df))
top_coefs <- top_coefs[-c(1)] #removing the intercept
# Generate balance stats for top 10 covariates
bal_df <- balance_table$Balance[top_coefs,]
bal_df
# Only 5 out of 10 of the top covariates are balanced post-matching

# Plot it
# There is no way to plot only a subset of variables, so instead of plotting top_coefs, we plot them all.
summary(matching_ps_att) %>% plot()
```

## Simulations

For this section, use select() function

Henderson/Chatfield argue that an improperly specified propensity score model can actually \textit{increase} the bias of the estimate. To demonstrate this, they simulate 800,000 different propensity score models by choosing different permutations of covariates. To investigate their claim, do the following:

\begin{itemize}
    \item Using as many simulations as is feasible (at least 10,000 should be ok, more is better!), randomly select the number of and the choice of covariates for the propensity score model.
    \item For each run, store the ATT, the proportion of covariates that meet the standardized mean difference $\leq .1$ threshold, and the mean percent improvement in the standardized mean difference. You may also wish to store the entire models in a list and extract the relevant attributes as necessary.
    \item Plot all of the ATTs against all of the balanced covariate proportions. You may randomly sample or use other techniques like transparency if you run into overplotting problems. Alternatively, you may use plots other than scatterplots, so long as you explore the relationship between ATT and the proportion of covariates that meet the balance threshold.
    \item Finally choose 10 random models and plot their covariate balance plots (you may want to use a library like \href{https://cran.r-project.org/web/packages/gridExtra/index.html}{gridExtra} to arrange these)
\end{itemize}

\textbf{Note: There are lots of post-treatment covariates in this dataset (about 50!)! You need to be careful not to include these in the pre-treatment balancing. Many of you are probably used to selecting or dropping columns manually, or positionally. However, you may not always have a convenient arrangement of columns, nor is it fun to type out 50 different column names. Instead see if you can use dplyr 1.0.0 functions to programatically drop post-treatment variables (\href{https://www.tidyverse.org/blog/2020/03/dplyr-1-0-0-select-rename-relocate/}{here} is a useful tutorial).}

```{r}
# Remove post-treatment covariates & outcome variable
ypsps_dropna_droplater = ypsps %>%
  select(!contains("1973") & !contains("1982")) %>%
  na.omit()

pre_treat_df = ypsps_dropna_droplater %>%
  select(!contains("student_ppnscal")) %>%
  select(!"college") # we don't want "college" to be picked as a covariate for college attendance

pre_covars_quant = ncol(pre_treat_df)
pre_covars = colnames(pre_treat_df)
```

```{r}
# Generate empty data frame to hold stats from random simulations
simulations_df = data.frame(matrix(ncol=3, nrow=1000))
colnames(simulations_df) = c("mean_improvement", "prop_threshold", "ATT")

# Generate empty nested list to hold models
simul_models = list()
# Simulate random selection of features 10k+ times
r <- nrow(simulations_df)
for (sim in 1:r) {
  
  # randomly select a quantity of pre-treatment covariates
  num_vars = sample(1:pre_covars_quant, 1)
  
  # randomly select covariates, save as "features", save in df, and in formula syntax form
  features = sample(x = pre_covars, size = num_vars)
  features_form = paste("college ~ ",paste(features,collapse=" + "))
  
  # run propensity score matching model, using nearest neighbor method, linear model, ATT estimand 
  random_ps_att <- matchit(formula = formula(features_form),
                           data = ypsps_dropna_droplater, # fullest data while omitting NAs and irrelevant columns
                           method = "nearest",
                           distance = "glm",
                           link = "logit",
                           estimand = "ATT",
                           replace = TRUE,
                           ratio = 2)
  
  # save model in df
  simul_models[[sim]] <- random_ps_att
  
  # generate summary information for the matching model, extract percent improvement, and save in df
  random_ps_att_sum = summary(random_ps_att)
  random_ps_att_sum_impr = random_ps_att_sum[["reduction"]]
  mean_improved = mean(random_ps_att_sum_impr[,1])
  simulations_df$mean_improvement[sim] <- mean_improved
  
  # create cobalt object to fetch the proportion of covariates meeting the balance threshold, and save this proportion to the df
  co_bal <- bal.tab(x = random_ps_att, thresholds = .1)
  prop_bal = co_bal[["Balanced.mean.diffs"]][["count"]][1] / (co_bal[["Balanced.mean.diffs"]][["count"]][1] + co_bal[["Balanced.mean.diffs"]][["count"]][2])
  simulations_df$prop_threshold[sim] = prop_bal
  
  # save the data from the matching model and use for linear, matched regression on the political participation outcome. From this, get the ATT and save it in the df.
  random_ps_att_data = match.data(random_ps_att)
  features_form2 = paste("student_ppnscal ~ college + ",paste(features,collapse=" + "))
  lm_ps_att = lm(formula = features_form2, 
                 data = random_ps_att_data, 
                 weights = weights)
  lm_ps_att_sum = summary(lm_ps_att)
  ATT_ps = lm_ps_att_sum$coefficients["college", "Estimate"]
  simulations_df$ATT[sim] <- ATT_ps
}
```

```{r}
# Plot ATT v. proportion
simulations_df %>%
  ggplot() +
  geom_smooth(aes(x = prop_threshold, y = ATT)) +
  ggtitle("Average Treatment Effect on Treated by Proportion of Covariates Meeting Balance Threshold") +
  xlab("Proportion of Covariates Meeting Threshold") +
  ylab("ATT")
```

```{r}
# 10 random covariate balance plots (hint try gridExtra)
random_models = c(sample(x=simul_models, size = 10))
for (m in random_models) {
  rando_plot <- summary(m) %>% plot()
}

# Note: ggplot objects are finnicky so ask for help if you're struggling to automatically create them; consider using functions!
```

```{r}
sum(simulations_df$prop_threshold > 0.5)
sum(simulations_df$prop_threshold > 0.65999)
sum(simulations_df$mean_improvement > 0)
hist(simulations_df$ATT)
```
## Questions

\begin{enumerate}
    \item \textbf{How many simulations resulted in models with a higher proportion of balanced covariates? Do you have any concerns about this?}
    \item \textbf{Your Answer}: Less than half of the simulated models had a majority of their variables balanced. Less than a quarter of models had 2/3s of their variables balanced. And, only about a quarter of models actually had their balance *improved* through the matching process. This shows that there are not plenty of easy-to-match subjects across all or most variables. The selection of variables is therefore very important, and that including *all* the relevant variables might produce a model that is not balanced enough to generate reliable results for this dataset.
    \item \textbf{Analyze the distribution of the ATTs. Do you have any concerns about this distribution?}
    \item \textbf{Your Answer:} It is not a normal distribution. And more importantly, more than 80% of models produce a positive ATT, showing that assigning propensity scores essentially at random, you're very likely to get a result that says that college augments later political activity. This supports the argument by the Berkeley grad students.
    \item \textbf{Do your 10 randomly chosen covariate balance plots produce similar numbers on the same covariates? Is it a concern if they do not?}
    \item \textbf{Your Answer:} Looking at student_GovtOpinion, student_demonstrate, and student_GovtCrook, it seems that they all make improvements in their balance post matching, but there is often an outlier model that improves by a much smaller margin its counterpart models. This might be a problem because it indicates that subjects who are matched in one model might actually be quite different from one another once we consider additional variables. 
\end{enumerate}

# Matching Algorithm of Your Choice

## Simulate Alternative Model

Henderson/Chatfield propose using genetic matching to learn the best weights for Mahalanobis distance matching. Choose a matching algorithm other than the propensity score (you may use genetic matching if you wish, but it is also fine to use the greedy or optimal algorithms we covered in lab instead). Repeat the same steps as specified in Section 4.2 and answer the following questions:

```{r}
install.packages("Matching")

library(Matching)
```

```{r}
# Generate empty data frame to hold stats from random simulations
# Run 500 simulations because 1000 took over an hour 
simulations_df5 = data.frame(matrix(ncol=3, nrow=500))
colnames(simulations_df5) = c("mean_improvement", "prop_threshold", "ATT")

# Generate empty nested list to hold models
simul_models5 = list()

# Simulate random selection of features 10k+ times
r5 <- nrow(simulations_df5)
for (sim in 1:r5) {
  
  # randomly select a quantity of pre-treatment covariates
  num_vars = sample(1:pre_covars_quant, 1)
  
  # randomly select covariates, save as "features", save in df, and in formula syntax form
  features = sample(x = pre_covars, size = num_vars)
  features_form = paste("college ~ ",paste(features,collapse=" + "))

  match_full_att <- matchit(formula = formula(features_form),
                           data = ypsps_dropna_droplater, # fullest data while omitting NAs and irrelevant columns 
                           method = "full", distance = "mahalanobis")

  # save model in df
  simul_models5[[sim]] <- match_full_att
  
  
    # generate summary information for the matching model, extract percent improvement, and save in df
  match_full_att_sum = summary(match_full_att)
  match_full_att_sum_impr = match_full_att_sum[["reduction"]]
  mean_improved = mean(match_full_att_sum_impr[,1])
  simulations_df5$mean_improvement[sim] <- mean_improved

    # create cobalt object to fetch the proportion of covariates meeting the balance threshold, and save this proportion to the df
  co_bal <- bal.tab(x = match_full_att, thresholds = .1)
  prop_bal = co_bal[["Balanced.mean.diffs"]][["count"]][1] / (co_bal[["Balanced.mean.diffs"]][["count"]][1] + co_bal[["Balanced.mean.diffs"]][["count"]][2])
  simulations_df5$prop_threshold[sim] = prop_bal
  
    #Fitmodels and save ATTs, proportion of balanced covariates, and mean percent balance improvement.
  match_full_att_data = match.data(match_full_att)
  features_form2 = paste("student_ppnscal ~ college + ",paste(features,collapse=" + "))
  lm_ps_att = lm(formula = features_form2, 
                 data = match_full_att_data, 
                 weights = weights)
  lm_ps_att_sum = summary(lm_ps_att)
  ATT_ps = lm_ps_att_sum$coefficients["college", "Estimate"]
  simulations_df5$ATT[sim] <- ATT_ps
}
```

```{r}
# Plot ATT v. proportion
simulations_df5 %>%
  ggplot() +
  geom_smooth(aes(x = prop_threshold, y = ATT)) +
  ggtitle("Average Treatment Effect on Treated by Proportion of Covariates Meeting Balance Threshold") +
  xlab("Proportion of Covariates Meeting Threshold") +
  ylab("ATT")
```

```{r}
# 10 random covariate balance plots (hint try gridExtra)
random_models5 = c(sample(x=simul_models, size = 10))
for (m in random_models5) {
  rando_plot5 <- summary(m) %>% plot()
}
```


```{r}
# Visualization for distributions of percent improvement
hist(simulations_df5$mean_improvement)
hist(simulations_df$mean_improvement)
```

## Questions

\begin{enumerate}
    \item \textbf{Does your alternative matching method have more runs with higher proportions of balanced covariates?}
    \item \textbf{Your Answer:} 
    \item \textbf{Use a visualization to examine the change in the distribution of the percent improvement in balance in propensity score matching vs. the distribution of the percent improvement in balance in your new method. Which did better? Analyze the results in 1-2 sentences.}
    \item \textbf{Your Answer:} The alternative matching method uses Mahalanobis matching and shows less variation in the mean improvement and a higher proportion of balanced covariates than the original simulation.  
\end{enumerate}

\textbf{Optional:} Looking ahead to the discussion questions, you may choose to model the propensity score using an algorithm other than logistic regression and perform these simulations again, if you wish to explore the second discussion question further.

# Discussion Questions

\begin{enumerate}
    \item Why might it be a good idea to do matching even if we have a randomized or as-if-random design?
    \item \textbf{Your Answer:} Matching is helpful in randomized or as-if-random designs if the covariates in the treatment and control groups are unbalanced. Therefore, matching methods will improve the balance of covariates and precision of the model.
    \item The standard way of estimating the propensity score is using a logistic regression to estimate probability of treatment. Given what we know about the curse of dimensionality, do you think there might be advantages to using other machine learning algorithms (decision trees, bagging/boosting forests, ensembles, etc.) to estimate propensity scores instead?
    \item \textbf{Your Answer:} The curse of dimensionality is when we organize our data into a high number of dimensions because phenomena do not occur at lower-dimensions. However, as the volume increases, the data become sparse and we end up with many empty cells across our covariates. Even though estimating propensity scores using logistic regression is standard, it imposes several parametric assumptions about the distribution of a population (e.g., independence of errors, linearity between the log-odds of the outcome and continuous variables). Other machine learning algorithms like random forest or decision trees are advantageous to estimate propensity scores because they are non-parametric and do not impose any assumptions about the distribution of the data.  
\end{enumerate}